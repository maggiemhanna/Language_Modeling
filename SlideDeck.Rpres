Language Model: Next Word Prediction
========================================================
author: Maggie Mhanna
date: 20/11/2017
autosize: true



Next Word Prediction
========================================================

The aim of the project is to develop an app - possibly using a novel algorithm - to predict the next word given a phrase or fragment of a sentence. A large corpus of data - consisting of English language content from tweets on Twitter, blogs and news groups was provided by the course team (and originally from http://www.corpora.heliohost.org/ ), in order to train and test prediction models. Two main models were investigated:

- N-gram models with a simple backoff 
- N-gram models with a Katz backoff 

Simple BackOff Model
========================================================

- A simple quadgram backoff approach, trained on collection of ngrams, was used to predict a subsequent word.
- With this approach, a subsequent word is predicted by using progressively shorter histories.
    + For example, let’s say you type “I love the” in order to see what the next predicted word is.
    + If the model contains a number of words that follow “I love the”, the most frequent terms are suggested.
    + If the model does not contain any words that follow “I love the”, the first word of the term is backed-off so that the model searches for words that follow “love the”.
    + If no words exist, the model searches for the most frequent word that follows “the”.
- We can optimize the memory by keeping only the first 5 most powerful predictions in each group of ngrams, (n-1)grams ... 

Katz BackOff Model
========================================================

- Katz's BackOff Model is useful in ngram language modeling to estimate the probability $P(w_{i}\mid w_{i-n+1}\cdots w_{i-1})$

- Probability of high-order N-gram is redistributed to lower-order N-gram by using Good-Turing Discounting.

- The equation for Katz's back-off model is:

${\displaystyle {\begin{aligned}&P_{bo}(w_{i}\mid w_{i-n+1}\cdots w_{i-1})\\={}&{\begin{cases}d_{w_{i-n+1}\cdots w_{i}}{\dfrac {C(w_{i-n+1}\cdots w_{i-1}w_{i})}{C(w_{i-n+1}\cdots w_{i-1})}}&{\text{if }}C(w_{i-n+1}\cdots w_{i})>k\\[10pt]\alpha _{w_{i-n+1}\cdots w_{i-1}}P_{bo}(w_{i}\mid w_{i-n+2}\cdots w_{i-1})&{\text{otherwise}}\end{cases}}\end{aligned}}}$

where $C(x)$ = number of times $x$ appears in training, $w_i$ = ith word in the given context, we choose $k=0$


The App
========================================================

- Perplexity is theoretically elegant but is inapplicable to unnormalized LMs or those with different vocabularies.
- Instead, a simple "AvgScore" calculates the percentage of times the next word is found among the 5 choices suggested by the model.

```{r echo=F}
ModelPerformance <- read.csv("final/ModelPerformance.csv")
knitr::kable(ModelPerformance, row.names = F)
```

- A detailed notebook explaining all models and steps can be found at http://rpubs.com/maggiemhanna/exploration_modeling_next_word_prediction

- The application is deployed on https://maggiemhanna.shinyapps.io/predictingnextword/ 

The App
========================================================

The application can be simply used by entering the text (with no size limit), the most powerful 5 possibilities of next words will then be shown.

![drawing](AppExample2.png)


