shiny::runApp('OneDrive - RENAULT/POC_DIAG_iDAV/POC_3/ShinyApp_cotech_2')
l <- 1
shiny::runApp('Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text/Product/LanguageModel')
runApp('Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text/Product/LanguageModel')
getwd()
runApp('Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text/LanguageModelProduct')
runApp('Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text/LanguageModelProduct')
rm(list = ls())
setwd("~/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text")
load(file = paste0("~/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text", file, "/data.Rdata"))
file <- "KatzBackOffModel"
load(file = paste0("~/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text", file, "/data.Rdata"))
paste0("~/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text", file, "/data.Rdata")
load(file = paste0("~/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text/", file, "/data.Rdata"))
source(file = paste0("~/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text/", file, "/Predictor.R"))
runApp('LanguageModelProduct')
?textOutput
wordcloud(c("akjhd", "sjdhqs"))
library(wordcloud)
wordcloud(c("akjhd", "sjdhqs"))
library(SnowballC)
wordcloud(c("akjhd", "sjdhqs"))
library(RColorBrewer)
wordcloud(c("akjhd", "sjdhqs"),  min.freq = 1,
max.words=300, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(c("akjhd", "sjdhqs"), freq = c(2, 1), min.freq = 1,
max.words=300, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(predictor("merry"), freq = 5:1, min.freq = 1,
max.words=300, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(Predictor("merry"), freq = 5:1, min.freq = 1,
max.words=300, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
if(!existsFunction("Predictor")) {
Predictor <- Katz_Predictor
rm(Katz_Predictor)
}
wordcloud(Predictor("merry"), freq = 5:1, min.freq = 1,
max.words=300, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(Predictor("merry"), freq = 5:1, min.freq = 1,
max.words=300, random.order=FALSE, rot.per=0.1,
colors=brewer.pal(8, "Dark2"))
wordcloud(Predictor("merry"), freq = 5:1, min.freq = 1,
max.words=300, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
wordcloud(Predictor("merry"), freq = 10:5, min.freq = 1,
max.words=300, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
10:5
wordcloud(Predictor("merry"), freq = 10:5, min.freq = 1,
max.words=300, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
wordcloud(Predictor("merry"), freq = 6:2, min.freq = 1,
max.words=300, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
wordcloud(Predictor("merry"), freq = 5:1, min.freq = 1,
max.words=300, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
wordcloud(Predictor("merry"), freq = 6:2, min.freq = 1,
max.words=300, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
?wordcloud
?renderPrint
runApp('LanguageModelProduct')
?wordcloud
runApp('LanguageModelProduct')
Predictor("my")
print(cat(Predictor("my"), sep = "\n")
)
runApp('LanguageModelProduct')
?radioButtons
?actionButton
runApp('LanguageModelProduct')
words = Predictor("merry")
words
words[2]
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
words
data.frame(Predictions = words)
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
?sidebarPanel
runApp('LanguageModelProduct')
?dataTableOutput
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
?textAreaInput
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
runApp('LanguageModelProduct')
rm(list = ls())
# Remove and clean all memory
rm(list = ls())
gc()
library(dplyr)
library(tm)
library(plyr)
library(stringr)
library(readr)
library(RWeka)
s_probability <- function(s){
if(nchar(s) > 0) {
s_ngrams <- c(NGramTokenizer(s, Weka_control(min=1, max=1))[1],
NGramTokenizer(s, Weka_control(min=2, max=2))[1],
NGramTokenizer(s, Weka_control(min=3, max=3))[1],
NGramTokenizer(s, Weka_control(min=4, max=4)))
s_ngrams <- s_ngrams[!is.na(s_ngrams)]
return(mean(sapply(s_ngrams, wc_probability, USE.NAMES = FALSE)))
} else {
return(NA)
}
}
wc_probability <- function(s_ngram) {
last_word <- word(s_ngram, -1)
input_text <- word(s_ngram, 1, -2)
choices <- Predictor(input_text)
if(last_word %in% choices) {
return(1)
} else {
return(0)
}
}
testset <- readLines('final/testset.txt')
knitr::opts_knit$set(root.dir = '/Users/az01661/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text')
# Remove and clean all memory
rm(list = ls())
gc()
library(dplyr)
library(tm)
library(plyr)
library(stringr)
library(readr)
library(RWeka)
s_probability <- function(s){
if(nchar(s) > 0) {
s_ngrams <- c(NGramTokenizer(s, Weka_control(min=1, max=1))[1],
NGramTokenizer(s, Weka_control(min=2, max=2))[1],
NGramTokenizer(s, Weka_control(min=3, max=3))[1],
NGramTokenizer(s, Weka_control(min=4, max=4)))
s_ngrams <- s_ngrams[!is.na(s_ngrams)]
return(mean(sapply(s_ngrams, wc_probability, USE.NAMES = FALSE)))
} else {
return(NA)
}
}
wc_probability <- function(s_ngram) {
last_word <- word(s_ngram, -1)
input_text <- word(s_ngram, 1, -2)
choices <- Predictor(input_text)
if(last_word %in% choices) {
return(1)
} else {
return(0)
}
}
testset <- readLines('final/testset.txt')
ModelPerformance <- data.frame(Model = character(), LoadTime = numeric(), AvgPredictionTime = numeric(), AvgScore = numeric(), ModelSize = character())
Examples <-c("you are so", "a lot of", "one of the", "out of the", "as well as", "going to be",
"the united states", "thanks for the", "it would be", "some of the")
for(file in list.files(pattern = "Model")){
## Calculating loading time
load.time.start <- Sys.time()
load(file = paste0(file, "/data.Rdata"))
source(file = paste0(file, "/Predictor.R"))
load.time <- Sys.time() - load.time.start
if(!existsFunction("Predictor")) {
Predictor <- Katz_Predictor
rm(Katz_Predictor)
}
## Calculating average prediction time
pred.time.start <- Sys.time()
sapply(Examples, Predictor)
pred.time <- Sys.time() - pred.time.start
pred.time <- pred.time/length(Examples)
objects.size <- sapply(ls()[grepl("^ngram", ls())],function(x){format(object.size(get(x)), units = "Mb")})
objects.size <- paste0(sum(parse_number(objects.size)), units = " Mb")
# testset is already shuffled
# let's use only 10 lines for now, as it takes too much time
score <- mean(sapply(testset[1:10], s_probability), na.rm = T)
ModelPerformance <- rbind(ModelPerformance, data.frame(Model = file, LoadTime = load.time, AvgPredictionTime = pred.time, AvgScore = score, ModelSize = objects.size))
rm(list=setdiff(ls(), c("ModelPerformance", "Examples", "testset", "s_probability", "wc_probability")))
gc()
}
ModelPerformance
ModelPerformance
file = "SimpleBackOffModel"
## Calculating loading time
load.time.start <- Sys.time()
load(file = paste0(file, "/data.Rdata"))
source(file = paste0(file, "/Predictor.R"))
load.time <- Sys.time() - load.time.start
if(!existsFunction("Predictor")) {
Predictor <- Katz_Predictor
rm(Katz_Predictor)
}
## Calculating average prediction time
pred.time.start <- Sys.time()
sapply(Examples, Predictor)
pred.time <- Sys.time() - pred.time.start
pred.time <- pred.time/length(Examples)
objects.size <- sapply(ls()[grepl("^ngram", ls())],function(x){format(object.size(get(x)), units = "Mb")})
objects.size <- paste0(sum(parse_number(objects.size)), units = " Mb")
# testset is already shuffled
# let's use only 10 lines for now, as it takes too much time
score <- mean(sapply(testset[1:10], s_probability), na.rm = T)
ModelPerformance <- rbind(ModelPerformance, data.frame(Model = file, LoadTime = load.time, AvgPredictionTime = pred.time, AvgScore = score, ModelSize = objects.size))
rm(list=setdiff(ls(), c("ModelPerformance", "Examples", "testset", "s_probability", "wc_probability")))
gc()
file = "SimpleBackOffModelOptimized"
## Calculating loading time
load.time.start <- Sys.time()
load(file = paste0(file, "/data.Rdata"))
source(file = paste0(file, "/Predictor.R"))
load.time <- Sys.time() - load.time.start
if(!existsFunction("Predictor")) {
Predictor <- Katz_Predictor
rm(Katz_Predictor)
}
## Calculating average prediction time
pred.time.start <- Sys.time()
sapply(Examples, Predictor)
pred.time <- Sys.time() - pred.time.start
pred.time <- pred.time/length(Examples)
objects.size <- sapply(ls()[grepl("^ngram", ls())],function(x){format(object.size(get(x)), units = "Mb")})
objects.size <- paste0(sum(parse_number(objects.size)), units = " Mb")
# testset is already shuffled
# let's use only 10 lines for now, as it takes too much time
score <- mean(sapply(testset[1:10], s_probability), na.rm = T)
ModelPerformance <- rbind(ModelPerformance, data.frame(Model = file, LoadTime = load.time, AvgPredictionTime = pred.time, AvgScore = score, ModelSize = objects.size))
rm(list=setdiff(ls(), c("ModelPerformance", "Examples", "testset", "s_probability", "wc_probability")))
gc()
knitr:kable(ModelPerformance)
ModelPerformance
write.csv(x = ModelPerformance, "final/ModelPerformance.csv")
runApp('Product')
runApp('Product')
load(file = paste0("~/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text/", file, "/data.Rdata"))
source(file = paste0("~/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text/", file, "/Predictor.R"))
ModelPerformance
file <- "SimpleBackOffModelOptimized"
load(file = paste0("~/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text/", file, "/data.Rdata"))
source(file = paste0("~/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text/", file, "/Predictor.R"))
bigramsearch
Predictor()
Predictor
if(!existsFunction("Predictor")) {
Predictor <- Katz_Predictor
rm(Katz_Predictor)
}
Predictor("")
Predictor("hi")
runApp('Product')
runApp('Product')
file <- "SimpleBackOffModel"
load(file = paste0("~/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text/", file, "/data.Rdata"))
source(file = paste0("~/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text/", file, "/Predictor.R"))
runApp('Product')
rm(list = ls())
unigramsearch <- function(nb_words = 5, remove_words = "") {
next_words <-  ngram.1 %>%
filter(!(Word %in% remove_words)) %>%
filter(row_number() <= nb_words) %>%
select(Word)
next_words <- as.vector(next_words$Word)
next_words
}
bigramsearch <- function(word1, nb_words = 5, remove_words = "") {
next_words <- ngram.2 %>%
filter(Wordone == word1) %>%
select(Wordtwo) %>%
filter(!(Wordtwo %in% remove_words)) %>%
filter(row_number() <= nb_words)
next_words <- as.vector(next_words$Wordtwo)
if(length(next_words) < nb_words)
next_words <- c(next_words, unigramsearch(nb_words - length(next_words), next_words))
next_words
}
trigramsearch <- function(word1, word2, nb_words = 5, remove_words = "") {
next_words <- ngram.3 %>%
filter(Wordone == word1, Wordtwo == word2) %>%
select(Wordthree) %>%
filter(!(Wordthree %in% remove_words)) %>%
filter(row_number() <= nb_words)
next_words <- as.vector(next_words$Wordthree)
if(length(next_words) < nb_words)
next_words <- c(next_words, bigramsearch(word2, nb_words - length(next_words), next_words))
next_words
}
fourgramsearch <- function(word1, word2, word3, nb_words = 5, remove_words = "") {
next_words <- ngram.4 %>%
filter(Wordone == word1,
Wordtwo == word2,
Wordthree == word3) %>%
select(Wordfour) %>%
filter(!(Wordfour %in% remove_words)) %>%
filter(row_number() <= nb_words)
next_words <- as.vector(next_words$Wordfour)
if(length(next_words) < nb_words)
next_words <- c(next_words, trigramsearch(word2, word3, nb_words - length(next_words), next_words))
next_words
}
Predictor <- function(text_input) {
# if(nchar(text_input) > 0) {
corpus_input <- VCorpus(VectorSource(text_input))
corpus_input<-tm_map(corpus_input, removeNumbers)
corpus_input<-tm_map(corpus_input, removePunctuation, preserve_intra_word_dashes = TRUE)
corpus_input<-tm_map(corpus_input, content_transformer(tolower))
text_input <- sapply(corpus_input, as.character)
text_input_list = strsplit(text_input, " ")[[1]]
# Predicting first word
if(length(text_input_list) == 0){
# We use unigrams to predict the next word
next_words <- unigramsearch()
}
if(length(text_input_list) == 1){
# We use bigrams to predict the next word
next_words <- bigramsearch(text_input_list[1])
}
if(length(text_input_list) == 2){
# We use trigrams to predict the next word
next_words <- trigramsearch(text_input_list[1], text_input_list[2])
}
if(length(text_input_list) >= 3){
# We use 4 grams to predict the next word
n = length(text_input_list)
next_words <- fourgramsearch(text_input_list[n-2], text_input_list[n-1], text_input_list[n])
}
next_words <- as.vector(next_words)
next_words
# }
}
dump(list = c("Predictor", "unigramsearch", "bigramsearch", "trigramsearch", "fourgramsearch"), file = "SimpleBackOffModel/Predictor.R")
rm(list = ls())
unigramsearch <- function(nb_words = 5, remove_words = "") {
next_words <-  ngram.1.sample %>%
filter(!(Word %in% remove_words)) %>%
filter(row_number() <= nb_words) %>%
select(Word)
next_words <- as.vector(next_words$Word)
next_words
}
bigramsearch <- function(word1, nb_words = 5, remove_words = "") {
next_words <- ngram.2.sample %>%
filter(Wordone == word1) %>%
select(Wordtwo) %>%
filter(!(Wordtwo %in% remove_words)) %>%
filter(row_number() <= nb_words)
next_words <- as.vector(next_words$Wordtwo)
if(length(next_words) < nb_words)
next_words <- c(next_words, unigramsearch(nb_words - length(next_words), next_words))
next_words
}
trigramsearch <- function(word1, word2, nb_words = 5, remove_words = "") {
next_words <- ngram.3.sample %>%
filter(Wordone == word1, Wordtwo == word2) %>%
select(Wordthree) %>%
filter(!(Wordthree %in% remove_words)) %>%
filter(row_number() <= nb_words)
next_words <- as.vector(next_words$Wordthree)
if(length(next_words) < nb_words)
next_words <- c(next_words, bigramsearch(word2, nb_words - length(next_words), next_words))
next_words
}
fourgramsearch <- function(word1, word2, word3, nb_words = 5, remove_words = "") {
next_words <- ngram.4.sample %>%
filter(Wordone == word1,
Wordtwo == word2,
Wordthree == word3) %>%
select(Wordfour) %>%
filter(!(Wordfour %in% remove_words)) %>%
filter(row_number() <= nb_words)
next_words <- as.vector(next_words$Wordfour)
if(length(next_words) < nb_words)
next_words <- c(next_words, trigramsearch(word2, word3, nb_words - length(next_words), next_words))
next_words
}
Predictor <- function(text_input) {
# if(nchar(text_input) > 0) {
corpus_input <- VCorpus(VectorSource(text_input))
corpus_input<-tm_map(corpus_input, removeNumbers)
corpus_input<-tm_map(corpus_input, removePunctuation, preserve_intra_word_dashes = TRUE)
corpus_input<-tm_map(corpus_input, content_transformer(tolower))
text_input <- sapply(corpus_input, as.character)
text_input_list = strsplit(text_input, " ")[[1]]
# Predicting first word
if(length(text_input_list) == 0){
# We use unigrams to predict the next word
next_words <- unigramsearch()
}
if(length(text_input_list) == 1){
# We use bigrams to predict the next word
next_words <- bigramsearch(text_input_list[1])
}
if(length(text_input_list) == 2){
# We use trigrams to predict the next word
next_words <- trigramsearch(text_input_list[1], text_input_list[2])
}
if(length(text_input_list) >= 3){
# We use 4 grams to predict the next word
n = length(text_input_list)
next_words <- fourgramsearch(text_input_list[n-2], text_input_list[n-1], text_input_list[n])
}
next_words <- as.vector(next_words)
next_words
# }
}
dump(list = c("Predictor", "unigramsearch", "bigramsearch", "trigramsearch", "fourgramsearch"), file = "SimpleBackOffModelOptimized/Predictor.R")
runApp('Product')
file <- "SimpleBackOffModel"
load(file = paste0("~/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text/", file, "/data.Rdata"))
source(file = paste0("~/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text/", file, "/Predictor.R"))
runApp('Product')
file <- "SimpleBackOffModelOptimized"
load(file = paste0("~/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text/", file, "/data.Rdata"))
source(file = paste0("~/Dropbox/3 Data Science Specialization/NLP Predicting Next Word in Text/", file, "/Predictor.R"))
runApp('Product')
# Remove and clean all memory
rm(list = ls())
gc()
library(dplyr)
library(tm)
library(plyr)
library(stringr)
library(readr)
library(RWeka)
s_probability <- function(s){
if(nchar(s) > 0) {
s_ngrams <- c(NGramTokenizer(s, Weka_control(min=1, max=1))[1],
NGramTokenizer(s, Weka_control(min=2, max=2))[1],
NGramTokenizer(s, Weka_control(min=3, max=3))[1],
NGramTokenizer(s, Weka_control(min=4, max=4)))
s_ngrams <- s_ngrams[!is.na(s_ngrams)]
return(mean(sapply(s_ngrams, wc_probability, USE.NAMES = FALSE)))
} else {
return(NA)
}
}
wc_probability <- function(s_ngram) {
last_word <- word(s_ngram, -1)
input_text <- word(s_ngram, 1, -2)
choices <- Predictor(input_text)
if(last_word %in% choices) {
return(1)
} else {
return(0)
}
}
testset <- readLines('final/testset.txt')
ModelPerformance <- data.frame(Model = character(), LoadTime = numeric(), AvgPredictionTime = numeric(), AvgScore = numeric(), ModelSize = character())
Examples <-c("you are so", "a lot of", "one of the", "out of the", "as well as", "going to be",
"the united states", "thanks for the", "it would be", "some of the")
for(file in list.files(pattern = "Model")){
## Calculating loading time
load.time.start <- Sys.time()
load(file = paste0(file, "/data.Rdata"))
source(file = paste0(file, "/Predictor.R"))
load.time <- Sys.time() - load.time.start
if(!existsFunction("Predictor")) {
Predictor <- Katz_Predictor
rm(Katz_Predictor)
}
## Calculating average prediction time
pred.time.start <- Sys.time()
sapply(Examples, Predictor)
pred.time <- Sys.time() - pred.time.start
pred.time <- pred.time/length(Examples)
objects.size <- sapply(ls()[grepl("^ngram", ls())],function(x){format(object.size(get(x)), units = "Mb")})
objects.size <- paste0(sum(parse_number(objects.size)), units = " Mb")
# testset is already shuffled
# let's use only 10 lines for now, as it takes too much time
score <- mean(sapply(testset[1:10], s_probability), na.rm = T)
ModelPerformance <- rbind(ModelPerformance, data.frame(Model = file, LoadTime = load.time, AvgPredictionTime = pred.time, AvgScore = score, ModelSize = objects.size))
rm(list=setdiff(ls(), c("ModelPerformance", "Examples", "testset", "s_probability", "wc_probability")))
gc()
}
ModelPerformance
write.csv(x = ModelPerformance, "final/ModelPerformance.csv")
ModelPerformance <- read.csv("final/ModelPerformance.csv")
runApp('Product')
runApp('Product')
file <- "SimpleBackOffModelOptimized"
load(file = paste0(file, "/data.Rdata"))
source(file = paste0(file, "/Predictor.R"))
runApp('Product')
rm(list = ls())
